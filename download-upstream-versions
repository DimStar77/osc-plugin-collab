#!/usr/bin/env python
# vim: set ts=4 sw=4 et: coding=UTF-8

#
# Copyright (c) 2008, Novell, Inc.
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:
#
#  * Redistributions of source code must retain the above copyright notice,
#    this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright notice,
#    this list of conditions and the following disclaimer in the documentation
#    and/or other materials provided with the distribution.
#  * Neither the name of the <ORGANIZATION> nor the names of its contributors
#    may be used to endorse or promote products derived from this software
#    without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
# POSSIBILITY OF SUCH DAMAGE.
#
#
# (Licensed under the simplified BSD license)
#
# Parts of this code comes from convert-to-tarball.py (in the releng GNOME
# svn module), which has no license. Therefore, this script is not 100%
# under the BSD license.
#
#
# Authors: Vincent Untz <vuntz@novell.com>
#

import os
import socket
import sys
import time

import ftplib
import optparse
from posixpath import join as posixjoin # Handy for URLs
import re
from sgmllib import SGMLParser
import urllib2
import urlparse

try:
    from xml.etree import cElementTree as ET
except ImportError:
    import cElementTree as ET

import feedparser

from dissector_util import *


DIR_PARENT = '/tmp/obs-dissector'

if os.getenv('OBS_DISSECTOR_DIR') and os.getenv('OBS_DISSECTOR_DIR') != '':
    DIR_PARENT=os.getenv('OBS_DISSECTOR_DIR')


#######################################################################


def _line_is_comment(line):
    return line.strip() == '' or line[0] == '#'


#######################################################################


# Fix some locations to point to what are really downloads.
def _location_fix(location):
    sf_jp = re.compile('^http://sourceforge.jp/projects/([^/]+)/downloads/([^/]+)/([^/]+)$')

    match = sf_jp.match(location)
    if match:
        # Unfortunate, but there's no other solution than to use a specific
        # mirror
        return 'http://globalbase.dl.sourceforge.jp/%s/%s/%s' % (match.group(1), match.group(2), match.group(3))

    return location


#######################################################################


class UpstreamDownloadError(Exception):
    def __init__(self, value):
        self.msg = value

    def __str__(self):
        return repr(self.msg)


#######################################################################


# comes from convert-to-tarball.py
class urllister(SGMLParser):
    def reset(self):
        SGMLParser.reset(self)
        self.urls = []

    def start_a(self, attrs):
        href = [v for k, v in attrs if k=='href']
        if href:
            self.urls.extend(href)


#######################################################################


class svnurllister(SGMLParser):
    def reset(self):
        SGMLParser.reset(self)
        self.urls = []

    def start_file(self, attrs):
        href = [v for k, v in attrs if k=='href']
        if href:
            self.urls.extend(href)


#######################################################################


# based on code from convert-to-tarball.py
def _get_files_from_http(url):
    obj = urllib2.build_opener()

    # Get the files
    usock = obj.open(url)
    parser = urllister()
    parser.feed(usock.read())
    usock.close()
    parser.close()
    files = parser.urls

    return (url, files)


#######################################################################


# based on code from convert-to-tarball.py
def _get_files_from_ftp(url):
    parsed_url = urlparse.urlparse(url)

    ftp = ftplib.FTP(parsed_url.hostname)
    ftp.login(parsed_url.username or 'anonymous', parsed_url.password or '')
    ftp.cwd(parsed_url.path)
    files = ftp.nlst()
    ftp.quit()

    return (url, files)


#######################################################################


# based on code from convert-to-tarball.py
def _get_files_from_subdir_http(url, limit):
    obj = urllib2.build_opener()
    # Note that we accept directories called 1.x.X
    good_dir = re.compile('^(([0-9]+|[xX])\.)*([0-9]+|[xX])/?$')
    def hasdirs(x): return good_dir.search(x)
    def fixdirs(x): return re.sub(r'^((?:(?:[0-9]+|[xX])\.)*)([0-9]+|[xX])/?$', r'\1\2', x)
    location = url
    # Follow 302 codes when retrieving URLs, speeds up conversion by 60sec
    redirect_location = location
    while True:
        # Get the files
        usock = obj.open(redirect_location)
        parser = urllister()
        parser.feed(usock.read())
        usock.close()
        parser.close()
        files = parser.urls

        # Check to see if we need to descend to a subdirectory
        newdirs = filter(hasdirs, files)
        newdirs = map(fixdirs, newdirs)
        if newdirs:
            newdir = _get_latest_version(newdirs, limit)
            # This is a weird case, that we handled for compiz-fusion:
            # if the dir contains a subdir with the same name, then we stop
            # FIXME: make this an option in the metadata?
            if newdir == os.path.basename(redirect_location):
                break
            if not newdir:
                break

            redirect_location = posixjoin(usock.url, newdir)
            location = posixjoin(location, newdir)
        else:
            break

    return (location, files)


#######################################################################


def _get_files_from_svn(url):
    obj = urllib2.build_opener()

    # Get the files
    usock = obj.open(url)
    parser = svnurllister()
    parser.feed(usock.read())
    usock.close()
    parser.close()
    files = parser.urls

    return (url, files)


#######################################################################


# comes from convert-to-tarball.py
def _bigger_version(a, b):
    a_nums = a.split('.')
    b_nums = b.split('.')
    num_fields = min(len(a_nums), len(b_nums))
    for i in range(0,num_fields):
        if   int(a_nums[i]) > int(b_nums[i]):
            return a
        elif int(a_nums[i]) < int(b_nums[i]):
            return b
    if len(a_nums) > len(b_nums):
        return a
    else:
        return b


def _setup_limit(limit):
    limit_data = None

    if not limit:
        pass
    elif limit == 'no-odd-unstable':
        pass
    elif limit[:4] == 'max|':
        limit_data = limit[4:]
        limit = 'max'
    else:
        print >>sys.stderr, 'Unsupported limit: %s' % limit
        limit = None

    return (limit, limit_data)

def _respect_limit(version, limit, limit_data):
    if not limit:
        return True
    elif limit == 'no-odd-unstable':
        split_version = version.split('.')
        if len(split_version) <= 1:
            # not enough version data, so let's just say yes
            return True

        try:
            return int(split_version[1]) % 2 == 0
        except:
            # second element is not an int. Let's just say yes
            return True

    elif limit == 'max':
        # if the limit is exactly the same as the version we're looking at,
        # _bigger_version will return what we want
        return limit_data == _bigger_version(limit_data, version) and version != limit_data

    else:
        return False


def _get_latest_version(versions, limit):
    (limit, limit_data) = _setup_limit(limit)

    biggest = None
    for version in versions:
        if _respect_limit(version, limit, limit_data):
            biggest = version
            break

    if not biggest:
        return None

    for version in versions[versions.index(biggest) + 1:]:
        if _respect_limit(version, limit, limit_data) and version == _bigger_version(biggest, version):
            biggest = version

    return biggest


#######################################################################


def _all_indexes(list, item, shift = 0):
    try:
        i = list.index(item)
        i += shift
    except ValueError:
        return []

    subresult = _all_indexes(list[i+1:], item, i + 1)
    subresult.append(i)
    return subresult


# based on code from convert-to-tarball.py
def _get_version_from_files(modulename, location, files, limit):
    # Only include tarballs for the given module
    tarballs = [file for file in files if modulename in file]

    re_tarball = r'^.*'+modulename+'[_-](([0-9]+[\.\-])*[0-9]+)\.(?:tar.*|t[bg]z2?)$'
    # Don't include -beta -installer -stub-installer and all kinds of
    # other weird-named tarballs
    tarballs = filter(lambda t: re.search(re_tarball, t), tarballs)

    versions = map(lambda t: re.sub(re_tarball, r'\1', t), tarballs)

    if not len(versions):
        raise UpstreamDownloadError('No versions found')

    version = _get_latest_version(versions, limit)

    if not version:
        raise UpstreamDownloadError('No version found respecting the limits')

    indexes = _all_indexes(versions, version)
    # the list is not in the right order, because of the way we build the list
    indexes.reverse()

    latest = [tarballs[index] for index in indexes]

    tarballs = None
    if not tarballs:
        tarballs = [file for file in latest if file.endswith('.tar.bz2')]
    if not tarballs:
        tarballs = [file for file in latest if file.endswith('.tar.gz')]
    if not tarballs:
        tarballs = [file for file in latest if file.endswith('.tbz2')]
    if not tarballs:
        tarballs = [file for file in latest if file.endswith('.tgz')]

    if not tarballs:
        raise UpstreamDownloadError('No tarballs found for version %s' % version)

    # at this point, all the tarballs we have are relevant, so just take the
    # first one
    tarball = tarballs[0]

    if urlparse.urlparse(tarball).scheme != '':
        # full URI
        location = tarball
    else:
        # remove files from location when we know it's not a directory
        if len(location) > 5 and location[-5:] in [ '.html' ]:
            last_slash = location.rfind('/')
            if last_slash != -1:
                location = location[:last_slash + 1]
        # add potentially missing slash to the directory
        if location[-1:] != '/':
            location = location + '/'
        location = urlparse.urljoin(location, tarball)

    return (location, version)


#######################################################################


def _get_version_from_sf_rss(modulename, id, limit):
    (limit, limit_data) = _setup_limit(limit)

    ids = id.split('|')
    url = 'http://sourceforge.net/export/rss2_projfiles.php?group_id=%s' % ids[0]
    if len(ids) > 1:
        url += '&package_id=%s' % ids[1]

    feed = feedparser.parse(url)

    version_re = re.compile('.* ((?:[0-9]+\.)*)([0-9]+) released.*')
    download_re = re.compile('<a href="([^"]*)">\[Download\]</a>')

    biggest = '0'
    location = None

    for entry in feed['entries']:
        version = version_re.sub(r'\1\2', entry.title)
        if version == entry.title:
            # could not apply regexp
            continue
        if not version == _bigger_version(version, biggest):
            continue
        if not _respect_limit(version, limit, limit_data):
            continue

        tarball_re = re.compile('%s-%s\.tar' % (modulename, version))
        match = tarball_re.search(entry.summary)
        if not match:
            continue

        match = download_re.search(entry.summary)
        if match:
            download_url = match.group(1)
        biggest = version
        location = download_url

    # if we have a link to the sf download page, look at this page
    if location:
        (location, files) = _get_files_from_http(location)

        good_files = [file for file in files if 'downloading.php' in file]

        tarballs = None
        if not tarballs:
            tarballs = [file for file in good_files if '.tar.bz2' in file]
        if not tarballs:
            tarballs = [file for file in good_files if '.tar.gz' in file]

        if tarballs:
            # let's take the first tarball
            url = urlparse.urlparse(tarballs[0])
            if url.path[0] == '/':
                path = url.path[1:]
            else:
                path = url.path
            if url.query:
                path = '%s?%s' % (path, url.query)
            location = 'http://sourceforge.net/%s' % path

            # this is now the page where we're redirected to the download...
            (location, files) = _get_files_from_http(location)

            good_files = [file for file in files if 'dl.sourceforge.net' in file]

            tarballs = None
            if not tarballs:
                tarballs = [file for file in good_files if '.tar.bz2' in file]
            if not tarballs:
                tarballs = [file for file in good_files if '.tar.gz' in file]

            if tarballs:
                # let's take the first tarball
                url = urlparse.urlparse(tarballs[0])
                if url.path[0] == '/':
                    path = url.path[1:]
                else:
                    path = url.path
                location = 'http://dl.sourceforge.net/%s' % path


    if biggest == '0' and location == None:
        biggest = None

    return (location, biggest)


#######################################################################


def _get_version_from_google_atom(name, limit):
    (limit, limit_data) = _setup_limit(limit)

    names = name.split('|')
    project = names[0]
    if len(names) > 1:
        tarball = names[1]
    else:
        tarball = project

    url = 'http://code.google.com/feeds/p/%s/downloads/basic' % project

    feed = feedparser.parse(url)

    version_re = re.compile('^\s*'+tarball+'[_-]((?:[0-9]+\.)*[0-9]+)\.tar.*')
    download_re = re.compile('<a href="([^"]*)">Download</a>')

    biggest = '0'
    location = None

    for entry in feed['entries']:
        match = version_re.match(entry.title)
        if not match:
            continue

        version = match.group(1)
        if not version == _bigger_version(version, biggest):
            continue
        if not _respect_limit(version, limit, limit_data):
            continue

        match = download_re.search(entry.content[0]['value'])
        if match:
            download_url = match.group(1)
        else:
            download_url = 'http://code.google.com/p/%s/downloads/list' % project
        biggest = version
        location = download_url

    if biggest == '0' and location == None:
        raise UpstreamDownloadError('No versions found')

    return (location, biggest)


#######################################################################


LP_NS = '{https://launchpad.net/rdf/launchpad#}'
RDF_NS = '{http://www.w3.org/1999/02/22-rdf-syntax-ns#}'

def _get_version_from_launchpad_series(project, limit, limit_data, series):
    url = 'https://launchpad.net/%s/%s/+rdf' % (project, series)
    release_re = re.compile('^/%s/%s/((?:[0-9]+\.)*[0-9]+)/\+rdf$' % (project, series))
    biggest = '0'

    fd = urllib2.urlopen(url)
    root = ET.parse(fd).getroot().find(LP_NS + 'ProductSeries')
    fd.close()

    for node in root.findall(LP_NS + 'release'):
        productrelease = node.find(LP_NS + 'ProductRelease')
        if not productrelease:
            continue
        specified = productrelease.find(LP_NS + 'specifiedAt')
        release = specified.get(RDF_NS + 'resource')
        match = release_re.match(release)
        if not match:
            continue
        version = match.group(1)

        if not _respect_limit(version, limit, limit_data):
            continue

        if version == _bigger_version(version, biggest):
            biggest = version

    # TODO: this is blocked by https://bugs.launchpad.net/bugs/268359
    location = None

    if biggest == '0':
        raise UpstreamDownloadError('No versions found')

    return (location, biggest)


def _get_version_from_launchpad(project, limit):
    (limit, limit_data) = _setup_limit(limit)

    url = 'https://launchpad.net/%s/+rdf' % project
    series_re = re.compile('^/%s/((?:[0-9]+\.)*[0-9]+)/\+rdf$' % project)
    series_biggest = '0'
    biggest = '0'

    fd = urllib2.urlopen(url)
    root = ET.parse(fd).getroot().find(LP_NS + 'Product')
    fd.close()

    # always try to look at trunk, before any branch
    try:
        (location_trunk, biggest_trunk) = _get_version_from_launchpad_series (project, limit, limit_data, 'trunk')
        found_trunk = True
    except UpstreamDownloadError:
        found_trunk = False

    for node in root.findall(LP_NS + 'series'):
        product = node.find(LP_NS + 'ProductSeries')
        if not product:
            continue
        specified = product.find(LP_NS + 'specifiedAt')
        series = specified.get(RDF_NS + 'resource')
        match = series_re.match(series)
        if not match:
            continue
        series_version = match.group(1)

        if not _respect_limit(series_version, limit, limit_data):
            continue

        if series_version == _bigger_version(series_version, series_biggest):
            series_biggest = series_version

    if series_biggest == '0':
        if found_trunk:
            (location_trunk, biggest_trunk)
        else:
            raise UpstreamDownloadError('No versions found')


    try:
        (location, biggest) = _get_version_from_launchpad_series (project, limit, limit_data, series_biggest)
    except UpstreamDownloadError:
        if found_trunk:
            return (location_trunk, biggest_trunk)
        else:
            raise UpstreamDownloadError('No versions found')

    if found_trunk:
        if biggest_trunk == _bigger_version(biggest, biggest_trunk):
            return (location_trunk, biggest_trunk)
        else:
            return (location, biggest)
    else:
        return (location, biggest)


#######################################################################


def get_upstream_version(modulename, method, additional_info, limit):
    if method not in [ 'upstream', 'ftpls', 'httpls', 'dualhttpls', 'subdirhttpls', 'svnls', 'sf', 'google', 'lp' ]:
        print >>sys.stderr, 'Unsupported method: %s' % method
        return (None, None)

    if method == 'upstream':
        return (None, '--')

    elif method == 'ftpls':
        (location, files) = _get_files_from_ftp(additional_info)
        return _get_version_from_files(modulename, location, files, limit)

    elif method == 'httpls':
        (location, files) = _get_files_from_http(additional_info)
        return _get_version_from_files(modulename, location, files, limit)

    elif method == 'dualhttpls':
        (url1, url2) = additional_info.split('|')
        (location1, files1) = _get_files_from_http(url1)
        (location2, files2) = _get_files_from_http(url2)
        try:
            (location1, version1) = _get_version_from_files(modulename, location1, files1, limit)
        except UpstreamDownloadError:
            (location1, version1) = (None, None)

        try:
            (location2, version2) = _get_version_from_files(modulename, location2, files2, limit)
        except UpstreamDownloadError:
            (location2, version2) = (None, None)

        if version1 and version2 and version1 == _bigger_version(version1, version2):
            return (location1, version1)
        elif version1 and version2:
            return (location2, version2)
        elif version1:
            return (location1, version1)
        elif version2:
            return (location2, version2)
        else:
            raise UpstreamDownloadError('No versions found')

    elif method == 'subdirhttpls':
        (location, files) = _get_files_from_subdir_http(additional_info, limit)
        return _get_version_from_files(modulename, location, files, limit)

    elif method == 'svnls':
        (location, files) = _get_files_from_svn(additional_info)
        return _get_version_from_files(modulename, location, files, limit)

    elif method == 'sf':
        return _get_version_from_sf_rss(modulename, additional_info, limit)

    elif method == 'google':
        return _get_version_from_google_atom(additional_info, limit)

    elif method == 'lp':
        return _get_version_from_launchpad(additional_info, limit)


#######################################################################


def parse_limits(limits_file):
    retval = {}

    if not os.path.exists(limits_file) or not os.path.isfile(limits_file):
        return retval

    file = open(limits_file)
    lines = file.readlines()
    file.close()

    for line in lines:
        if _line_is_comment(line):
            continue

        data = line[:-1].split(':', 2)
        retval[data[0]] = data[1]

    return retval


#######################################################################


def parse_data(data_file):
    retval = {}

    if not os.path.exists(data_file) or not os.path.isfile(data_file):
        return retval

    file = open(data_file)
    lines = file.readlines()
    file.close()

    for line in lines:
        if _line_is_comment(line):
            continue

        data = line[:-1].split(':', 3)
        if data[0] != 'nonfgo':
            continue

        if data[2] != '':
            version = data[2]
        else:
            version = None

        if data[3] != '':
            location = data[3]
        else:
            location = None

        retval[data[1]] = (version, location)

    return retval


#######################################################################


def main(args):
    parser = optparse.OptionParser()

    parser.add_option('--debug', dest='debug',
                      help='only handle the argument as input and output the result')
    parser.add_option('--directory', dest='dir',
                      help='directory where to find data and save data')
    parser.add_option('--save-file', dest='save',
                      help='path to the file where the results will be written')
    parser.add_option('--upstream-limits', dest='upstream_limits',
                      help='path to the upstream limits data file')
    parser.add_option('--upstream-tarballs', dest='upstream_tarballs',
                      help='path to the upstream tarballs data file')
    parser.add_option('--fast-update', action='store_true',
                      default=False, dest='fast_update',
                      help='when available, use old saved data instead of looking for new data (limits will be ignored)')
    parser.add_option('--use-old-as-fallback', action='store_true',
                      default=False, dest='fallback',
                      help='if available, use old saved data as a fallback for when we cannot find new data (limits will be ignored for the fallback case)')
    parser.add_option('--only-if-old', action='store_true',
                      default=False, dest='only_if_old',
                      help='execute only if the pre-existing result file is older than 10 hours')

    (options, args) = parser.parse_args()

    fallback_data = {}

    if options.dir:
        directory = options.dir
    else:
        directory = DIR_PARENT

    if options.upstream_limits:
        limit_file = options.upstream_limits
    else:
        limit_file = os.path.join(directory, 'upstream-limits.txt')

    limits = parse_limits(limit_file)

    if options.debug:
        lines = [ options.debug + '\n' ]
        out = sys.stdout

    else:
        if options.upstream_tarballs:
            upstream_file = options.upstream_tarballs
        else:
            upstream_file = os.path.join(directory, 'upstream-tarballs.txt')

        if options.save:
            save_file = options.save
        else:
            save_file = os.path.join(directory, 'versions-non-fgo')

        if not os.path.exists(upstream_file):
            print >>sys.stderr, 'Upstream data file %s does not exist.' % upstream_file
            sys.exit(1)
        elif not os.path.isfile(upstream_file):
            print >>sys.stderr, 'Upstream data file %s is not a regular file.' % upstream_file
            sys.exit(1)

        if os.path.exists(save_file):
            if not os.path.isfile(save_file):
                print >>sys.stderr, 'Save file %s is not a regular file.' % save_file
                sys.exit(1)
            if options.only_if_old:
                stats = os.stat(save_file)
                # Quit if it's less than 10-hours old
                if time.time() - stats.st_mtime < 3600 * 10:
                    sys.exit(0)

            if options.fallback or options.fast_update:
                fallback_data = parse_data(save_file)
        else:
            safe_mkdir_p(os.path.dirname(save_file))

        file = open(upstream_file)
        lines = file.readlines()
        file.close()

        out = open(save_file, 'w')

    # The default timeout is just too long. Use 10 seconds instead.
    socket.setdefaulttimeout(10)

    for line in lines:
        if _line_is_comment(line):
            continue

        (location, version) = (None, None)

        (modulename, method, additional_info) = line[:-1].split(':', 2)

        if options.fast_update and fallback_data.has_key(modulename) and fallback_data[modulename][0]:
            # fast update: we don't download data if we have something in cache
            pass
        else:
            if limits.has_key(modulename):
                limit = limits[modulename]
            else:
                limit = None

            try:
                (location, version) = get_upstream_version(modulename, method, additional_info, limit)
            except urllib2.URLError, e:
                print >>sys.stderr, 'Error when downloading information about %s: %s' % (modulename, e)
            except urllib2.HTTPError, e:
                print >>sys.stderr, 'Error when downloading information about %s: server sent %s' % (modulename, e.code)
            except ftplib.all_errors, e:
                print >>sys.stderr, 'Error when downloading information about %s: %s' % (modulename, e)
            except UpstreamDownloadError, e:
                print >>sys.stderr, 'No matching tarball found for %s: %s' % (modulename, e.msg)

        if fallback_data.has_key(modulename):
            fallback_version = fallback_data[modulename][0]
            fallback_location = fallback_data[modulename][1]

            if not version and not location:
                version = fallback_version
                location = fallback_location
            elif not version and location == fallback_location:
                version = fallback_version
            elif not location and version == fallback_version:
                location = fallback_location

        if version == '--':
            cat = 'upstream'
        else:
            cat = 'nonfgo'

        if location:
            location = _location_fix(location)

        if version and location:
            out.write('%s:%s:%s:%s\n' % (cat, modulename, version, location))
        elif version:
            out.write('%s:%s:%s:\n' % (cat, modulename, version))
        elif location:
            out.write('%s:%s::%s\n' % (cat, modulename, location))
        else:
            out.write('%s:%s::\n' % (cat, modulename))

    if not options.debug:
        out.close()


if __name__ == '__main__':
    try:
      main(sys.argv)
    except KeyboardInterrupt:
      pass
