#!/usr/bin/env python
# vim: set ts=4 sw=4 et: coding=UTF-8

#
# Copyright (c) 2008, Novell, Inc.
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:
#
#  * Redistributions of source code must retain the above copyright notice,
#    this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright notice,
#    this list of conditions and the following disclaimer in the documentation
#    and/or other materials provided with the distribution.
#  * Neither the name of the <ORGANIZATION> nor the names of its contributors
#    may be used to endorse or promote products derived from this software
#    without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
# POSSIBILITY OF SUCH DAMAGE.
#
#
# (Licensed under the simplified BSD license)
#
# Authors: Vincent Untz <vuntz@novell.com>
#

# TODO:
#  + Support checking out a project/package in a already existing checkout.
#    This implies that we handle an "update" operation, in case the package
#    is already checked out.
#    For a project, it's hard since we can't know if a package should be
#    removed or not (since it's possible to download only a subset of all
#    packages in a project; so the fact that a package is in the directory but
#    has not been checked out doesn't mean it doesn't exist in the project). So
#    we might end up with cruft.
#
#  + Add a feature to automatically detect collaboration branches.

import os
import sys

import bisect
import optparse
import shutil
import socket
import tempfile
import time
import urllib
import urllib2

import Queue
import threading

try:
    from xml.etree import cElementTree as ET
except ImportError:
    import cElementTree as ET

from osc import conf
from osc import core
from osc import oscerr

from dissector_util import *
from osc_extend import *

# We will check out everything in CHECKOUT_DIR_PARENT/$some_dir where $some_dir
# is based on CHECKOUT_DIR_PREFIX and some random data.
#
# Beware: if changing this, be really careful since the directories can get
# removed by this script. So you don't want to touch directories that contain
# precious data.
CHECKOUT_DIR_PARENT = '/tmp/obs-dissector'
CHECKOUT_DIR_PREFIX = 'obs-co-'

if os.getenv('OBS_DISSECTOR_DIR') and os.getenv('OBS_DISSECTOR_DIR') != '':
    CHECKOUT_DIR_PARENT=os.getenv('OBS_DISSECTOR_DIR')

# This is the directory (as a symlink) that can be used by outside programs
# when the checkout is complete. This means it will always contain complete &
# consistent data.
CHECKOUT_DIR_STABLE = os.path.join(CHECKOUT_DIR_PARENT, CHECKOUT_DIR_PREFIX + 'stable')

# Number of threads to use at a time
THREADS_MAX = 10

# Timeout for sockets
SOCKET_TIMEOUT = 30

# Debug output?
USE_DEBUG = False

#######################################################################


conf_initialized = False


def osc_conf_init():
    global conf_initialized

    if not conf_initialized:
        try:
            conf.get_config()
            conf_initialized = True
        except oscerr.NoConfigfile, e:
            print >>sys.stderr, e.msg
            sys.exit(1)


#######################################################################


def debug_thread(context, state, indent = '', use_remaining = False):
    global USE_DEBUG

    if not USE_DEBUG:
        return

    # compatibility with old versions of python (< 2.6)
    if hasattr(threading.currentThread(), 'name'):
        name = threading.currentThread().name
    else:
        name = threading.currentThread().getName()

    if context == 'main':
        print '%s%s: %s' % (indent, name, state)
        return

    try:
        safe_mkdir_p('obs-checkout-debug')
        fout = open(os.path.join('obs-checkout-debug', name), 'a')

        # ignore indent since we write in files
        fout.write('[%s] %s %s\n' % (context, time.strftime("%H:%M:%S", time.localtime()), state))

        if use_remaining:
            remaining = ''
            for i in threading.enumerate():
                remaining += i.name + ', '
            fout.write('Remaining: %s\n' % (remaining,))
        fout.close()
    except Exception, e:
        print >>sys.stderr, 'Exception in debug_thread: %s' % (e,)


def socket_closer_thread_run(obs_checkout, empty_event):
    # compatibility with old versions of python (< 2.6)
    if hasattr(empty_event, 'is_set'):
        empty_is_set = empty_event.is_set
    else:
        empty_is_set = empty_event.isSet

    while True:
        if empty_is_set():
            break

        obs_checkout.socket_timeouts_acquire()

        # Find the socket that is supposed to be closed the first, so we can
        # monitor it
        while True:
            if not len(obs_checkout.socket_timeouts):
                (max_time, current_socket) = (0, None)
                break

            (max_time, current_socket, url) = obs_checkout.socket_timeouts[0]

            if time.time() + SOCKET_TIMEOUT + 1 < max_time:
                debug_thread('monitor', 'closing socket for %s (too far)' % url)
                # close this socket: the max time is way too high
                current_socket.close()
                obs_checkout.socket_timeouts.remove((max_time, current_socket, url))
            else:
                break

        obs_checkout.socket_timeouts_release()

        # There's a socket to monitor, let's just do it
        if max_time > 0:

            while time.time() < max_time:
                time.sleep(1)
                # This is not thread-safe, but it can only go from False to
                # True.
                # If the value is still False, then we'll just try another
                # time (and worst case: we exit the loop because of the
                # timeout, but then we acquire the lock so the end will be
                # thread-safe: we won't close it twice).
                # If the value is True, then it's really closed anyway.
                if not current_socket.fp or current_socket.fp.closed:
                    break

            obs_checkout.socket_timeouts_acquire()
            if time.time() >= max_time:
                debug_thread('monitor', 'closing socket for %s (timed out)' % url)
                current_socket.close()
            if (max_time, current_socket, url) in obs_checkout.socket_timeouts:
                 obs_checkout.socket_timeouts.remove((max_time, current_socket, url))
            obs_checkout.socket_timeouts_release()

        else:
            # There's no socket to monitor at the moment, so we wait for one to
            # appear or for the notification of the end of the work.
            # We use less than the socket timeout value as timeout so we are
            # sure to not start too late the monitoring of the next socket (so
            # we don't allow a socket to stay more than its timeout).
            empty_event.wait(SOCKET_TIMEOUT / 2)


def obs_checkout_thread_run(obs_checkout):
    try:
        while True:
            debug_thread('thread_loop', 'start loop', use_remaining = True)
            if obs_checkout.queue.empty():
                break

            debug_thread('thread_loop', 'getting work...')
            # we don't want to block: the queue is filled at the beginning and
            # once it's empty, then it means we're done. So we want the
            # exception to happen.
            (project, package) = obs_checkout.queue.get(block = False)
            debug_thread('main', 'starting %s/%s' % (project, package))

            try:
                debug_thread('thread_loop', 'work = %s/%s' % (project, package))
                obs_checkout.checkout_package(project, package)
                debug_thread('thread_loop', 'work done')
            except Exception, e:
                print >>sys.stderr, 'Exception in worker thread for %s/%s: %s' % (project, package, e)

            obs_checkout.queue.task_done()
            debug_thread('thread_loop', 'end loop', use_remaining = True)
    except Queue.Empty:
        pass
    debug_thread('thread_loop', 'exit loop', use_remaining = True)


#######################################################################


class ObsCheckout:
    def __init__(self, dest_dir, cache_dir = None, no_update = False, threads = THREADS_MAX):
        osc_conf_init()

        self.dest_dir = dest_dir
        if dest_dir != cache_dir:
            self.cache_dir = cache_dir
        else:
            print >>sys.stderr, 'Ignoring specified cache directory that is also the destination directory'
        self.no_update = no_update
        self.threads = threads
        self.queue = None
        self.socket_timeouts = []
        self.socket_timeouts_lock = None


    def socket_timeouts_acquire(self):
        if self.socket_timeouts_lock:
            debug_thread('lock', 'acquiring lock')
            self.socket_timeouts_lock.acquire()
            debug_thread('lock', 'acquired lock')


    def socket_timeouts_release(self):
        if self.socket_timeouts_lock:
            self.socket_timeouts_lock.release()
            debug_thread('lock', 'released lock')


    def _download_url_to_file(self, url, file):
        fin = None
        fout = None
        timeout = 0
        try:
            debug_thread('url', 'start %s (timeout = %d)' % (url, socket.getdefaulttimeout()), ' ')
            fin = urllib2.urlopen(url)
            debug_thread('url', 'opened', ' ')

            self.socket_timeouts_acquire()
            timeout = time.time() + SOCKET_TIMEOUT
            bisect.insort(self.socket_timeouts, (timeout, fin, url))
            self.socket_timeouts_release()

            fout = open(file, 'w')

            while True:
                bytes = fin.read(500 * 1024)
                if len(bytes) == 0:
                    break
                fout.write(bytes)
            fout.close()

            self.socket_timeouts_acquire()
            if (timeout, fin, url) in self.socket_timeouts:
                self.socket_timeouts.remove((timeout, fin, url))
            fin.close()
            self.socket_timeouts_release()

            debug_thread('url', 'done', ' ')
        except Exception, e:
            debug_thread('url', 'exception: %s' % (e,), ' ')

            self.socket_timeouts_acquire()
            if (timeout, fin, url) in self.socket_timeouts:
                self.socket_timeouts.remove((timeout, fin, url))
            if fin:
                fin.close()
            self.socket_timeouts_release()

            if fout:
                fout.close()
            raise e


    def _get_file_metadata_from_cache(self, project, package, filename):
        if not self.cache_dir:
            return (None, None)

        files = os.path.join(self.cache_dir, project, package, '_files-expanded')
        if not os.path.exists(files):
            files = os.path.join(self.cache_dir, project, package, '_files')
            if not os.path.exists(files):
                return (None, None)

        try:
            root = ET.parse(files).getroot()
        except SyntaxError:
            return (None, None)

        for node in root.findall('entry'):
            if node.get('name') == filename:
                return (node.get('md5'), node.get('mtime'))

        return (None, None)


    def _get_file(self, project, package, filename, md5, mtime, revision = None, try_again = True):
        package_dir = os.path.join(self.dest_dir, project, package)
        destfile = os.path.join(package_dir, filename)

        # first try to copy the file from the cache
        (cached_md5, cached_mtime) = self._get_file_metadata_from_cache(project, package, filename)
        if md5 == cached_md5 and mtime == cached_mtime:
            cached_file = os.path.join(self.cache_dir, project, package, filename)
            if os.path.exists(cached_file):
                shutil.copyfile(cached_file, destfile)
                return

        # couldn't copy the file from the cache, so download it
        try:
            query = None
            if revision:
                query = { 'rev': revision }
            url = core.makeurl(conf.config['apiurl'], ['public', 'source', project, package, urllib.pathname2url(filename)], query=query)
            self._download_url_to_file(url, destfile)

        except (urllib2.HTTPError, urllib2.URLError), e:
            if try_again:
                self._get_file(project, package, filename, destfile, revision, False)
                return
            else:
                if hasattr(e, 'msg'):
                    detail = e.msg
                else:
                    detail = e
                print >>sys.stderr, 'Could not get file %s for %s from %s: %s' % (filename, package, project, detail)


    def _get_files_metadata(self, project, package, save_basename, revision = None, try_again = True):
        package_dir = os.path.join(self.dest_dir, project, package)
        filename = os.path.join(package_dir, save_basename)

        # if we already have the files metadata, this means we've already been
        # there in some way (because we're downloading multiple times something
        # in openSUSE:Factory, eg -- it can happen if a package exists in both
        # GNOME:Factory and mozilla:Factory)
        # In this case, we have nothing to do.
        if os.path.exists(filename):
            return None

        # download files metadata
        try:
            query = None
            if revision:
                query = { 'rev': revision }
            url = core.makeurl(conf.config['apiurl'], ['public', 'source', project, package], query=query)
            self._download_url_to_file(url, filename)

        except urllib2.HTTPError, e:
            if e.code == 404:
                print >>sys.stderr, 'Package %s doesn\'t exist in %s.' % (package, project)
            else:
                if try_again:
                    return self._get_files_metadata(project, package, save_basename, revision, False)
                elif revision:
                    print >>sys.stderr, 'Cannot download metadata of %s from %s with specified revision: %s' % (package, project, e.msg)
                else:
                    print >>sys.stderr, 'Cannot download metadata of %s from %s: %s' % (package, project, e.msg)
            return None
        except urllib2.URLError, e:
            if try_again:
                return self._get_files_metadata(project, package, save_basename, revision, False)
            elif revision:
                print >>sys.stderr, 'Cannot download metadata of %s from %s with specified revision: %s' % (package, project, e)
            else:
                print >>sys.stderr, 'Cannot download metadata of %s from %s: %s' % (package, project, e)
            return None

        try:
            return ET.parse(filename).getroot()
        except SyntaxError, e:
            if try_again:
                os.unlink(filename)
                return self._get_files_metadata(project, package, save_basename, revision, False)
            elif revision:
                print >>sys.stderr, 'Cannot parse metadata of %s from %s with specified revision: %s' % (package, project, e.msg)
            else:
                print >>sys.stderr, 'Cannot parse metadata of %s from %s: %s' % (package, project, e.msg)
            return None


    def checkout_package(self, project, package):
        # TODO: handle updates (if not self.no_update):
        # if the directory already exists, list files in there so we can know
        # what to remove afterwards.
        # Also, we need to change _get_file to look at the current downloaded
        # file.
        package_dir = os.path.join(self.dest_dir, project, package)
        safe_mkdir_p(package_dir)

        # find files we're interested in from the metadata
        root = self._get_files_metadata(project, package, '_files')
        if not root:
            return

        # detect if the package is a link package
        linkinfos_nb = len(root.findall('linkinfo'))
        linkinfo = EasyLinkinfo()
        if linkinfos_nb == 1:
            linkinfo.read(root.find('linkinfo'))
        elif linkinfos_nb > 1:
            print >>sys.stderr, 'Ignoring link in %s from %s: more than one <linkinfo>' % (package, project)

        # this will be None if it's not a link that matters and else, this will
        # be the right value to expand the link
        revision = linkinfo.xsrcmd5

        if linkinfo.needshandling():
            # download the _link file first. This makes it possible to know if
            # the project has a delta compared to the target of the link
            for node in root.findall('entry'):
                filename = node.get('name')
                md5 = node.get('md5')
                mtime = node.get('mtime')
                # download .spec files
                if filename == '_link':
                    self._get_file(project, package, filename, md5, mtime)

            # download the metadata of the expanded package
            root = self._get_files_metadata(project, package, '_files-expanded', revision)
            if not root:
                return

        # look at all files and download what might be interesting
        for node in root.findall('entry'):
            filename = node.get('name')
            md5 = node.get('md5')
            mtime = node.get('mtime')
            # download .spec files
            if filename.endswith('.spec'):
                self._get_file(project, package, filename, md5, mtime, revision)


    def run(self, use_threads = True):
        if self.socket_timeouts != []:
            print >>sys.stderr, 'Internal error: list of socket timeouts is not empty before running'
            return
        # queue is empty or does not exist: it could be that the requested
        # project does not exist
        if not self.queue:
            return

        debug_thread('main', 'queue has %d items' % self.queue.qsize())

        if use_threads and self.threads > 1:
            # Architecture with threads:
            #  + we fill a queue with all the tasks that have to be done
            #  + the main thread does nothing until the queue is empty
            #  + we create a bunch of threads that will take the tasks from the
            #    queue
            #  + we create a monitor thread that ensures that the socket
            #    connections from the other threads don't hang forever. The
            #    issue is that those threads use urllib2, and urllib2 will
            #    remove the timeout from the underlying socket. (see
            #    socket.makefile() documentation)
            #  + there's an event between the main thread and the monitor
            #    thread to announce to the monitor thread that the queue is
            #    empty and that it can leave.
            #  + once the queue is empty:
            #    - the helper threads all exit since there's nothing left to do
            #    - the main thread is waken up and sends an event to the
            #      monitor thread. It waits for it to exit.
            #    - the monitor thread receives the event and exits.
            #    - the main thread can continue towards the end of the process.

            # this is used to signal the monitor thread it can exit
            empty_event = threading.Event()
            # this is the lock for the data shared between the threads
            self.socket_timeouts_lock = threading.Lock()

            monitor = threading.Thread(target=socket_closer_thread_run, args=(self, empty_event))
            monitor.start()

            thread_args = (self,)
            for i in range(min(self.threads, self.queue.qsize())):
                t = threading.Thread(target=obs_checkout_thread_run, args=thread_args)
                t.start()

            self.queue.join()
            # tell the monitor thread to quit and wait for it
            empty_event.set()
            monitor.join()
        else:
            try:
                while not self.queue.empty():
                    (project, package) = self.queue.get(block = False)
                    debug_thread('main', 'starting %s/%s' % (project, package))
                    self.checkout_package(project, package)
            except Queue.Empty:
                pass


    def _queue_packages_from_project(self, project, packages):
        if not self.queue:
            # queue for tasks
            self.queue = Queue.Queue()

        for package in packages:
            self.queue.put((project, package))


    def queue_checkout_projects_packages_existing_in_project(self, projects, existing_in_project, try_again = True):
        try:
            packages = core.meta_get_packagelist(conf.config['apiurl'], existing_in_project)
        except (urllib2.HTTPError, urllib2.URLError), e:
            if try_again:
                self.checkout_project_packages_existing_in_project(project, existing_in_project, False)
            else:
                if hasattr(e, 'msg'):
                    detail = e.msg
                else:
                    detail = e
                print >>sys.stderr, 'Ignoring packages from %s for project %s: %s' % (existing_in_project, project, detail)
            return

        for project in projects:
            self._queue_packages_from_project(project, packages)


    def queue_checkout_project(self, project, try_again = True):
        try:
            packages = core.meta_get_packagelist(conf.config['apiurl'], project)
        except (urllib2.HTTPError, urllib2.URLError), e:
            if try_again:
                self.checkout_project(project, False)
            else:
                if hasattr(e, 'msg'):
                    detail = e.msg
                else:
                    detail = e
                print >>sys.stderr, 'Ignoring project %s: %s' % (project, e.msg)
            return

        self._queue_packages_from_project(project, packages)


    def queue_checkout_devel_projects(self, project, try_again = True):
        devel_projects = set()

        fin = None
        try:
            url = core.makeurl(conf.config['apiurl'], ['search', 'package'], ['match=%s' % urllib.quote('@project=\'%s\'' % project)])
            fin = urllib2.urlopen(url)
            collection = ET.parse(fin).getroot()
            for package in collection.findall('package'):
                devel = package.find('devel')
                if devel != None:
                    devel_project = devel.get('project')
                    if devel_project and devel_project != project:
                        devel_projects.add(devel_project)

            fin.close()

        except (urllib2.HTTPError, urllib2.URLError, SyntaxError), e:
            if fin:
                fin.close()

            if try_again:
                self.queue_checkout_devel_projects(project, False)
            else:
                if hasattr(e, 'msg'):
                    detail = e.msg
                else:
                    detail = e
                print >>sys.stderr, 'Ignoring devel projects for project %s: %s' % (project, detail)
            return

        for devel_project in devel_projects:
            self.queue_checkout_project(devel_project)


#######################################################################


def get_stable_directory():
    if os.path.lexists(CHECKOUT_DIR_STABLE):
        if os.path.islink(CHECKOUT_DIR_STABLE):
            return os.readlink(CHECKOUT_DIR_STABLE)
        else:
            return CHECKOUT_DIR_STABLE
    else:
        return None


def make_directory_stable(directory):
    if os.path.lexists(CHECKOUT_DIR_STABLE):
        if os.path.islink(CHECKOUT_DIR_STABLE):
            old_stable_directory = os.readlink(CHECKOUT_DIR_STABLE)
            os.unlink(CHECKOUT_DIR_STABLE)
        else:
            shutil.rmtree(CHECKOUT_DIR_STABLE)
    else:
        old_stable_directory = None

    os.symlink(directory, CHECKOUT_DIR_STABLE)

    # Do this after symlinking the new directory: doing it before would just
    # delay the access to the latest data
    if old_stable_directory and os.path.exists(old_stable_directory):
        shutil.rmtree(old_stable_directory)


#######################################################################


def main(args):
    global USE_DEBUG

    parser = optparse.OptionParser()

    parser.add_option('--get-stable-directory', action='store_true',
                      default=False, dest='get_stable_directory',
                      help='output the path to the current stable directory')
    parser.add_option('--get-new-directory', action='store_true',
                      default=False, dest='get_new_directory',
                      help='output the path of a new directory ready to be used')
    parser.add_option('--make-directory-stable', dest='make_directory_stable',
                      help='make the specified directory the stable directory')

    parser.add_option('--debug', action='store_true', dest='debug', default=USE_DEBUG)
    parser.add_option('--threads', type='int', dest='threads', default=THREADS_MAX)

    parser.add_option('--cache-directory', dest='cache_dir',
                      help='cache directory containing previous checkouts')
    parser.add_option('--destination-directory', dest='dest_dir',
                      help='directory where the project will be checked out')
    parser.add_option('--no-update', action='store_true',
                      default=False, dest='no_update',
                      help='when checking out something that already exists, just assume it is up-to-date instead of updating it')
    parser.add_option('--project', metavar='PROJECT', action='append',
                      default=[], dest='projects',
                      help='project(s) to check out')
    parser.add_option('--packages-of', metavar='PROJECT', action='append',
                      default=[], dest='packages_of',
                      help='only check out packages also existing in specified project(s)')
    parser.add_option('--devel-projects-of', metavar='PROJECT', action='append',
                      default=[], dest='devel_projects_of',
                      help='check out the devel projects used in specified project(s)')

    (options, args) = parser.parse_args()

    USE_DEBUG = options.debug

    # Check options validity

    conflicting_options = 0
    if options.get_stable_directory:
        conflicting_options = conflicting_options + 1
    if options.get_new_directory:
        conflicting_options = conflicting_options + 1
    if options.make_directory_stable:
        conflicting_options = conflicting_options + 1
    if options.cache_dir or options.dest_dir or options.projects or options.packages_of or options.devel_projects_of:
        conflicting_options = conflicting_options + 1

    if conflicting_options == 0:
        print >>sys.stderr, 'No option specified'
        sys.exit(1)
    elif conflicting_options > 1:
        print >>sys.stderr, 'Conflicting options specified'
        sys.exit(1)

    if options.packages_of and options.devel_projects_of:
        print >>sys.stderr, 'Conflicting options specified'
        sys.exit(1)

    if (options.cache_dir or options.dest_dir or options.packages_of) and not options.projects and not options.devel_projects_of:
        print >>sys.stderr, 'No project specified'
        sys.exit(1)

    # Stable/New directory handling

    if options.get_stable_directory:
        stable_dir = get_stable_directory()
        if stable_dir:
            print stable_dir
        sys.exit(0)

    if options.get_new_directory:
        new_dir = tempfile.mkdtemp('', CHECKOUT_DIR_PREFIX, CHECKOUT_DIR_PARENT)
        safe_mkdir_p(new_dir)
        print new_dir
        sys.exit(0)

    if options.make_directory_stable:
        make_directory_stable(options.make_directory_stable)
        sys.exit(0)

    # Checking out

    if not options.dest_dir:
        dest_dir = CHECKOUT_DIR_STABLE
        print 'No destination directory specified. Using %s' % CHECKOUT_DIR_STABLE
    else:
        dest_dir = options.dest_dir

    # set default timeout to SOCKET_TIMEOUT seconds to not hang forever
    socket.setdefaulttimeout(SOCKET_TIMEOUT)

    co = ObsCheckout(dest_dir, options.cache_dir, options.no_update, options.threads)

    if options.packages_of:
        for packages_of_one in options.packages_of:
            co.queue_checkout_projects_packages_existing_in_project(options.projects, options.packages_of_one)
    else:
        for project in options.projects:
            co.queue_checkout_project(project)
        for devel_projects_of_one in options.devel_projects_of:
            co.queue_checkout_devel_projects(devel_projects_of_one)

    co.run()


if __name__ == '__main__':
    try:
      main(sys.argv)
    except KeyboardInterrupt:
      pass
