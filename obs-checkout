#!/usr/bin/env python
# vim: set ts=4 sw=4 et: coding=UTF-8

#
# Copyright (c) 2008, Novell, Inc.
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:
#
#  * Redistributions of source code must retain the above copyright notice,
#    this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright notice,
#    this list of conditions and the following disclaimer in the documentation
#    and/or other materials provided with the distribution.
#  * Neither the name of the <ORGANIZATION> nor the names of its contributors
#    may be used to endorse or promote products derived from this software
#    without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
# POSSIBILITY OF SUCH DAMAGE.
#
#
# (Licensed under the simplified BSD license)
#
# Authors: Vincent Untz <vuntz@novell.com>
#

# TODO:
#  + Support checking out a project/package in a already existing checkout.
#    This implies that we handle an "update" operation, in case the package
#    is already checked out.
#    For a project, it's hard since we can't know if a package should be
#    removed or not (since it's possible to download only a subset of all
#    packages in a project; so the fact that a package is in the directory but
#    has not been checked out doesn't mean it doesn't exist in the project). So
#    we might end up with cruft.
#
#  + Add a feature to automatically detect collaboration branches.

import os
import sys

import optparse
import shutil
import socket
import tempfile
import urllib2

try:
    from xml.etree import cElementTree as ET
except ImportError:
    import cElementTree as ET

from osc import conf
from osc import core
from osc import oscerr

from dissector_util import *
from osc_extend import *

# We will check out everything in CHECKOUT_DIR_PARENT/$some_dir where $some_dir
# is based on CHECKOUT_DIR_PREFIX and some random data.
#
# Beware: if changing this, be really careful since the directories can get
# removed by this script. So you don't want to touch directories that contain
# precious data.
CHECKOUT_DIR_PARENT = '/tmp/obs-dissector'
CHECKOUT_DIR_PREFIX = 'obs-co-'

if os.getenv('OBS_DISSECTOR_DIR') and os.getenv('OBS_DISSECTOR_DIR') != '':
    CHECKOUT_DIR_PARENT=os.getenv('OBS_DISSECTOR_DIR')

# This is the directory (as a symlink) that can be used by outside programs
# when the checkout is complete. This means it will always contain complete &
# consistent data.
CHECKOUT_DIR_STABLE = os.path.join(CHECKOUT_DIR_PARENT, CHECKOUT_DIR_PREFIX + 'stable')


#######################################################################


conf_initialized = False


def osc_conf_init():
    global conf_initialized

    if not conf_initialized:
        try:
            conf.get_config()
            conf_initialized = True
        except oscerr.NoConfigfile, e:
            print >>sys.stderr, e.msg
            sys.exit(1)


#######################################################################


class ObsCheckout:
    def __init__(self, dest_dir, cache_dir = None, no_update = False, warn_non_links = False):
        osc_conf_init()

        self.dest_dir = dest_dir
        if dest_dir != cache_dir:
            self.cache_dir = cache_dir
        else:
            print >>sys.stderr, 'Ignoring specified cache directory that is also the destination directory'
        self.no_update = no_update
        self.warn_non_links = warn_non_links


    def _get_file_metadata_from_cache(self, project, package, filename):
        if not self.cache_dir:
            return (None, None)

        files = os.path.join(self.cache_dir, project, package, '_files-expanded')
        if not os.path.exists(files):
            files = os.path.join(self.cache_dir, project, package, '_files')
            if not os.path.exists(files):
                return (None, None)

        try:
            root = ET.parse(files).getroot()
        except SyntaxError:
            return (None, None)

        for node in root.findall('entry'):
            if node.get('name') == filename:
                return (node.get('md5'), node.get('mtime'))

        return (None, None)


    def _get_file(self, project, package, filename, md5, mtime, revision = None, try_again = True):
        package_dir = os.path.join(self.dest_dir, project, package)
        destfile = os.path.join(package_dir, filename)

        # first try to copy the file from the cache
        (cached_md5, cached_mtime) = self._get_file_metadata_from_cache(project, package, filename)
        if md5 == cached_md5 and mtime == cached_mtime:
            cached_file = os.path.join(self.cache_dir, project, package, filename)
            if os.path.exists(cached_file):
                shutil.copyfile(cached_file, destfile)
                return

        # couldn't copy the file from the cache, so download it
        try:
            core.get_source_file(conf.config['apiurl'], project, package, filename, destfile, revision)
        except urllib2.HTTPError, e:
            if try_again:
                self._get_file(project, package, filename, destfile, revision, False)
                return
            else:
                print >>sys.stderr, 'Could not get file %s for %s from %s: %s' % (filename, package, project, e.msg)
        except urllib2.URLError, e:
            if try_again:
                self._get_file(project, package, filename, destfile, revision, False)
                return
            else:
                print >>sys.stderr, 'Could not get file %s for %s from %s: %s' % (filename, package, project, e)


    def _get_files_metadata(self, project, package, save_basename, revision = None, try_again = True):
        package_dir = os.path.join(self.dest_dir, project, package)
        filename = os.path.join(package_dir, save_basename)

        # if we already have the files metadata, this means we've already been
        # there in some way (because we're downloading multiple times something
        # in openSUSE:Factory, eg -- it can happen if a package exists in both
        # GNOME:Factory and mozilla:Factory)
        # In this case, we have nothing to do.
        if os.path.exists(filename):
            return None

        # download files metadata
        try:
            metadata = core.show_files_meta(conf.config['apiurl'], project, package, revision)
        except urllib2.HTTPError, e:
            if e.code == 404:
                print >>sys.stderr, 'Package %s doesn\'t exist in %s.' % (package, project)
            else:
                if try_again:
                    return self._get_files_metadata(project, package, save_basename, revision, False)
                elif revision:
                    print >>sys.stderr, 'Cannot download metadata of %s from %s with specified revision: %s' % (package, project, e.msg)
                else:
                    print >>sys.stderr, 'Cannot download metadata of %s from %s: %s' % (package, project, e.msg)
            return None
        except urllib2.URLError, e:
            if try_again:
                return self._get_files_metadata(project, package, save_basename, revision, False)
            elif revision:
                print >>sys.stderr, 'Cannot download metadata of %s from %s with specified revision: %s' % (package, project, e)
            else:
                print >>sys.stderr, 'Cannot download metadata of %s from %s: %s' % (package, project, e)
            return None

        f = open(filename, 'w')
        f.write(''.join(metadata))
        f.close()

        try:
            return ET.parse(filename).getroot()
        except SyntaxError, e:
            if try_again:
                os.unlink(filename)
                return self._get_files_metadata(project, package, save_basename, revision, False)
            elif revision:
                print >>sys.stderr, 'Cannot parse metadata of %s from %s with specified revision: %s' % (package, project, e.msg)
            else:
                print >>sys.stderr, 'Cannot parse metadata of %s from %s: %s' % (package, project, e.msg)
            return None


    def checkout_package(self, project, package):
        # TODO: handle updates (if not self.no_update):
        # if the directory already exists, list files in there so we can know
        # what to remove afterwards.
        # Also, we need to change _get_file to look at the current downloaded
        # file.
        package_dir = os.path.join(self.dest_dir, project, package)
        safe_mkdir_p(package_dir)

        # find files we're interested in from the metadata
        root = self._get_files_metadata(project, package, '_files')
        if not root:
            return

        # detect if the package is a link package
        linkinfos_nb = len(root.findall('linkinfo'))
        linkinfo = EasyLinkinfo()
        if linkinfos_nb == 1:
            linkinfo.read(root.find('linkinfo'))
        elif linkinfos_nb > 1:
            print >>sys.stderr, 'Ignoring link in %s from %s: more than one <linkinfo>' % (package, project)

        # this will be None if it's not a link that matters and else, this will
        # be the right value to expand the link
        revision = linkinfo.xsrcmd5

        if linkinfo.haserror():
            print >>sys.stderr, 'Link in %s from %s has an error: %s' % (package, project, linkinfo.error)
        elif self.warn_non_links and not linkinfo.islink():
            print >>sys.stderr, 'Package %s from %s is not a link.' % (package, project)
        elif self.warn_non_links and linkinfo.isexpanded():
            print >>sys.stderr, 'Package %s from %s is an expanded link.' % (package, project)

        if linkinfo.needshandling():
            # download the _link file first. This makes it possible to know if
            # the project has a delta compared to the target of the link
            for node in root.findall('entry'):
                filename = node.get('name')
                md5 = node.get('md5')
                mtime = node.get('mtime')
                # download .spec files
                if filename == '_link':
                    self._get_file(project, package, filename, md5, mtime)

            # download the metadata of the expanded package
            root = self._get_files_metadata(project, package, '_files-expanded', revision)
            if not root:
                return

        # look at all files and download what might be interesting
        for node in root.findall('entry'):
            filename = node.get('name')
            md5 = node.get('md5')
            mtime = node.get('mtime')
            # download .spec files
            if filename.endswith('.spec'):
                self._get_file(project, package, filename, md5, mtime, revision)


    def _checkout_packages_from_project(self, project, packages):
        for package in packages:
            self.checkout_package(project, package)


    def checkout_project_packages_existing_in_project(self, project, existing_in_project, try_again = True):
        try:
            packages = core.meta_get_packagelist(conf.config['apiurl'], existing_in_project)
        except urllib2.HTTPError, e:
            if try_again:
                self.checkout_project_packages_existing_in_project(project, existing_in_project, False)
            else:
                print >>sys.stderr, 'Ignoring packages from %s for project %s: %s' % (existing_in_project, project, e.msg)
            return
        except urllib2.URLError, e:
            if try_again:
                self.checkout_project_packages_existing_in_project(project, existing_in_project, False)
            else:
                print >>sys.stderr, 'Ignoring packages from %s for project %s: %s' % (existing_in_project, project, e)
            return

        self._checkout_packages_from_project(project, packages)


    def checkout_project(self, project, try_again = True):
        try:
            packages = core.meta_get_packagelist(conf.config['apiurl'], project)
        except urllib2.HTTPError, e:
            if try_again:
                self.checkout_project(project, False)
            else:
                print >>sys.stderr, 'Ignoring project %s: %s' % (project, e.msg)
            return
        except urllib2.URLError, e:
            if try_again:
                self.checkout_project(project, False)
            else:
                print >>sys.stderr, 'Ignoring project %s: %s' % (project, e)
            return

        self._checkout_packages_from_project(project, packages)


#######################################################################


def get_stable_directory():
    if os.path.lexists(CHECKOUT_DIR_STABLE):
        if os.path.islink(CHECKOUT_DIR_STABLE):
            return os.readlink(CHECKOUT_DIR_STABLE)
        else:
            return CHECKOUT_DIR_STABLE
    else:
        return None


def make_directory_stable(directory):
    if os.path.lexists(CHECKOUT_DIR_STABLE):
        if os.path.islink(CHECKOUT_DIR_STABLE):
            old_stable_directory = os.readlink(CHECKOUT_DIR_STABLE)
            os.unlink(CHECKOUT_DIR_STABLE)
        else:
            shutil.rmtree(CHECKOUT_DIR_STABLE)
    else:
        old_stable_directory = None

    os.symlink(directory, CHECKOUT_DIR_STABLE)

    # Do this after symlinking the new directory: doing it before would just
    # delay the access to the latest data
    if old_stable_directory and os.path.exists(old_stable_directory):
        shutil.rmtree(old_stable_directory)


#######################################################################


def main(args):
    parser = optparse.OptionParser()

    parser.add_option('--get-stable-directory', action='store_true',
                      default=False, dest='get_stable_directory',
                      help='output the path to the current stable directory')
    parser.add_option('--get-new-directory', action='store_true',
                      default=False, dest='get_new_directory',
                      help='output the path of a new directory ready to be used')
    parser.add_option('--make-directory-stable', dest='make_directory_stable',
                      help='make the specified directory the stable directory')

    parser.add_option('--cache-directory', dest='cache_dir',
                      help='cache directory containing previous checkouts')
    parser.add_option('--destination-directory', dest='dest_dir',
                      help='directory where the project will be checked out')
    parser.add_option('--no-update', action='store_true',
                      default=False, dest='no_update',
                      help='when checking out something that already exists, just assume it is up-to-date instead of updating it')
    parser.add_option('--warn-if-not-unexpanded-link', action='store_true',
                      default=False, dest='warn_non_links',
                      help='print a warning for packages that are not unexpanded links')
    parser.add_option('--packages-of', dest='packages_of',
                      help='only check out packages also existing in specified project')
    parser.add_option('--project', dest='project',
                      help='project to check out')

    (options, args) = parser.parse_args()

    # Check options validity

    conflicting_options = 0
    if options.get_stable_directory:
        conflicting_options = conflicting_options + 1
    if options.get_new_directory:
        conflicting_options = conflicting_options + 1
    if options.make_directory_stable:
        conflicting_options = conflicting_options + 1
    if options.cache_dir or options.dest_dir or options.packages_of or options.project:
        conflicting_options = conflicting_options + 1

    if conflicting_options == 0:
        print >>sys.stderr, 'No option specified'
        sys.exit(1)
    elif conflicting_options > 1:
        print >>sys.stderr, 'Conflicting options specified'
        sys.exit(1)

    if (options.cache_dir or options.dest_dir or options.packages_of) and not options.project:
        print >>sys.stderr, 'No project specified'
        sys.exit(1)

    # Stable/New directory handling

    if options.get_stable_directory:
        stable_dir = get_stable_directory()
        if stable_dir:
            print stable_dir
        sys.exit(0)

    if options.get_new_directory:
        new_dir = tempfile.mkdtemp('', CHECKOUT_DIR_PREFIX, CHECKOUT_DIR_PARENT)
        safe_mkdir_p(new_dir)
        print new_dir
        sys.exit(0)

    if options.make_directory_stable:
        make_directory_stable(options.make_directory_stable)
        sys.exit(0)

    # Checking out

    if not options.dest_dir:
        dest_dir = CHECKOUT_DIR_STABLE
        print 'No destination directory specified. Using %s' % CHECKOUT_DIR_STABLE
    else:
        dest_dir = options.dest_dir

    # set default timeout to 30 seconds to not hang forever
    socket.setdefaulttimeout(30)

    co = ObsCheckout(dest_dir, options.cache_dir, options.no_update, options.warn_non_links)

    if options.packages_of:
        co.checkout_project_packages_existing_in_project(options.project, options.packages_of)
    else:
        co.checkout_project(options.project)


if __name__ == '__main__':
    try:
      main(sys.argv)
    except KeyboardInterrupt:
      pass
